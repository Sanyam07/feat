{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Feat \u00b6 Feat is a feature engineering automation tool that learns new representations of raw data to improve classifier and regressor performance. The underlying methods are based on Pareto optimization and evolutionary computation to search the space of possible transformations. Feat wraps around a user-chosen ML method and provides a set of representations that give the best performance for that method. Each individual in Feat\u2019s population is its own data representation. Feat uses the Shogun C++ ML toolbox to fit models. Check out the documentation for installation and examples. Acknowledgments \u00b6 This method is being developed to study human disease in the Epistasis Lab at UPenn . License \u00b6 GNU GPLv3","title":"Introduction"},{"location":"#feat","text":"Feat is a feature engineering automation tool that learns new representations of raw data to improve classifier and regressor performance. The underlying methods are based on Pareto optimization and evolutionary computation to search the space of possible transformations. Feat wraps around a user-chosen ML method and provides a set of representations that give the best performance for that method. Each individual in Feat\u2019s population is its own data representation. Feat uses the Shogun C++ ML toolbox to fit models. Check out the documentation for installation and examples.","title":"Feat"},{"location":"#acknowledgments","text":"This method is being developed to study human disease in the Epistasis Lab at UPenn .","title":"Acknowledgments"},{"location":"#license","text":"GNU GPLv3","title":"License"},{"location":"api_c/","text":"Feat C++ API \u00b6 This is the C++ API for Feat . Back to main documentation Quick links \u00b6 Feat Parameters Evaluation Selection Variation Population","title":"Feat C++ API"},{"location":"api_c/#feat-c-api","text":"This is the C++ API for Feat . Back to main documentation","title":"Feat C++ API"},{"location":"api_c/#quick-links","text":"Feat Parameters Evaluation Selection Variation Population","title":"Quick links"},{"location":"contributing/","text":"Contributing \u00b6 Please follow the Github flow guidelines for contributing to this project. In general, this is the approach: Fork the repo into your own repository and clone it locally. git clone https://github.com/my_user_name/feat Have an idea for a code change. Checkout a new branch with an appropriate name. git checkout -b my_new_change Make your changes. Commit your changes to the branch. git commit -m \"adds my new change\" Check that your branch has no conflict with Feat\u2019s master branch by merging the master branch from the upstream repo. git remote add upstream https://github.com/lacava/feat git fetch upstream git merge upstream/master Fix any conflicts and commit. git commit -m \"Merges upstream master\" Push the branch to your forked repo. git push origin my_new_change Go to either Github repo and make a new Pull Request for your forked branch. Be sure to reference any relevant issues.","title":"Contribute"},{"location":"contributing/#contributing","text":"Please follow the Github flow guidelines for contributing to this project. In general, this is the approach: Fork the repo into your own repository and clone it locally. git clone https://github.com/my_user_name/feat Have an idea for a code change. Checkout a new branch with an appropriate name. git checkout -b my_new_change Make your changes. Commit your changes to the branch. git commit -m \"adds my new change\" Check that your branch has no conflict with Feat\u2019s master branch by merging the master branch from the upstream repo. git remote add upstream https://github.com/lacava/feat git fetch upstream git merge upstream/master Fix any conflicts and commit. git commit -m \"Merges upstream master\" Push the branch to your forked repo. git push origin my_new_change Go to either Github repo and make a new Pull Request for your forked branch. Be sure to reference any relevant issues.","title":"Contributing"},{"location":"install/","text":"Installing \u00b6 To see our installation process from scratch, check out the Travis install file . Feat depends on the Eigen matrix library for C++ as well as the Shogun ML library. Both come in easy packages that work across platforms. If you need Eigen and Shogun, follow the instructions in Dependencies . Feat uses cmake to build. It uses the typical set of instructions: git clone https://github.com/lacava/feat # clone the repo cd feat # enter the directory ./configure # this runs \"mkdir build; cd build; cmake .. \" ./install # this runs \"make -C build VERBOSE=1 -j8\" Python wrapper \u00b6 The python wrapper is installed using setuptools as follows: cd python python setup.py install Dependencies \u00b6 Eigen \u00b6 Eigen is a header only package. We need Eigen 3 or greater. Debian/Ubuntu \u00b6 On Debian systems, you can grab the package: sudo apt-get install libeigen3-dev You can also download the headers and put them somewhere. Then you just have to tell cmake where they are with the environmental variable EIGEN3_INCLUDE_DIR . Example: # grab Eigen 3.3.4 wget \"http://bitbucket.org/eigen/eigen/get/3.3.4.tar.gz\" tar xzf 3.3.4.tar.gz mkdir eigen-3.3.4 mv eigen-eigen*/* eigen-3.3.4 # set an environmental variable to tell cmake where Eigen is export EIGEN3_INCLUDE_DIR=\"$(pwd)/eigen-3.3.4/\" Shogun \u00b6 You don\u2019t have to compile Shogun, just download the binaries. Their install guide is good. . We\u2019ve listed two of the options here. Anaconda \u00b6 A good option for Anaconda users is the Shogun Anaconda package. If you use conda, you can get what you need by conda install -c conda-forge shogun-cpp If you do this, you need cmake to find Anaconda\u2019s library and include directories. Set these two variables: export SHOGUN_LIB=/home/travis/miniconda/lib/ export SHOGUN_DIR=/home/travis/miniconda/include/ Debian/Ubuntu \u00b6 You can also get the Shogun packages: sudo add-apt-repository ppa:shogun-toolbox/nightly -y sudo apt-get update -y sudo apt-get install -qq --force-yes --no-install-recommends libshogun18 sudo apt-get install -qq --force-yes --no-install-recommends libshogun-dev Running the tests \u00b6 This is totally optional! If you want to run the tests, you need to install Google Test . A useful guide to doing so is available here . Then you can use cmake to build the tests. From the repo root, ./configure tests # builds the test Makefile make -C build tests # compiles the tests ./build/tests # runs the tests","title":"Installation"},{"location":"install/#installing","text":"To see our installation process from scratch, check out the Travis install file . Feat depends on the Eigen matrix library for C++ as well as the Shogun ML library. Both come in easy packages that work across platforms. If you need Eigen and Shogun, follow the instructions in Dependencies . Feat uses cmake to build. It uses the typical set of instructions: git clone https://github.com/lacava/feat # clone the repo cd feat # enter the directory ./configure # this runs \"mkdir build; cd build; cmake .. \" ./install # this runs \"make -C build VERBOSE=1 -j8\"","title":"Installing"},{"location":"install/#python-wrapper","text":"The python wrapper is installed using setuptools as follows: cd python python setup.py install","title":"Python wrapper"},{"location":"install/#dependencies","text":"","title":"Dependencies"},{"location":"install/#eigen","text":"Eigen is a header only package. We need Eigen 3 or greater.","title":"Eigen"},{"location":"install/#debianubuntu","text":"On Debian systems, you can grab the package: sudo apt-get install libeigen3-dev You can also download the headers and put them somewhere. Then you just have to tell cmake where they are with the environmental variable EIGEN3_INCLUDE_DIR . Example: # grab Eigen 3.3.4 wget \"http://bitbucket.org/eigen/eigen/get/3.3.4.tar.gz\" tar xzf 3.3.4.tar.gz mkdir eigen-3.3.4 mv eigen-eigen*/* eigen-3.3.4 # set an environmental variable to tell cmake where Eigen is export EIGEN3_INCLUDE_DIR=\"$(pwd)/eigen-3.3.4/\"","title":"Debian/Ubuntu"},{"location":"install/#shogun","text":"You don\u2019t have to compile Shogun, just download the binaries. Their install guide is good. . We\u2019ve listed two of the options here.","title":"Shogun"},{"location":"install/#anaconda","text":"A good option for Anaconda users is the Shogun Anaconda package. If you use conda, you can get what you need by conda install -c conda-forge shogun-cpp If you do this, you need cmake to find Anaconda\u2019s library and include directories. Set these two variables: export SHOGUN_LIB=/home/travis/miniconda/lib/ export SHOGUN_DIR=/home/travis/miniconda/include/","title":"Anaconda"},{"location":"install/#debianubuntu_1","text":"You can also get the Shogun packages: sudo add-apt-repository ppa:shogun-toolbox/nightly -y sudo apt-get update -y sudo apt-get install -qq --force-yes --no-install-recommends libshogun18 sudo apt-get install -qq --force-yes --no-install-recommends libshogun-dev","title":"Debian/Ubuntu"},{"location":"install/#running-the-tests","text":"This is totally optional! If you want to run the tests, you need to install Google Test . A useful guide to doing so is available here . Then you can use cmake to build the tests. From the repo root, ./configure tests # builds the test Makefile make -C build tests # compiles the tests ./build/tests # runs the tests","title":"Running the tests"},{"location":"api/c/launchpad/","text":"Full C++ API , built using Doxygen","title":"C++ API"},{"location":"api/python/api_py/","text":"Coming soon. For now, refer to the source.","title":"Python API"},{"location":"examples/archive/","text":"Using the Archive \u00b6 In this example, we apply Feat to a regression problem and visualize the archive of representations. Note: this code uses the Penn ML Benchmark Suite to fetch data. You can install it using pip install pmlb . Also available as a notebook Training Feat \u00b6 First, we import the data and create a train-test split. from pmlb import fetch_data from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error as mse import numpy as np dataset='690_visualizing_galaxy' X, y = fetch_data(dataset,return_X_y=True) X_t,X_v, y_t, y_v = train_test_split(X,y,train_size=0.75,test_size=0.25,random_state=42) Then we set up a Feat instance and train the model, storing the final archive. from feat import Feat # fix the random state random_state=11314 fest = Feat(pop_size=500, # train 500 representations gens=100, # maximum of 200 generations max_time=60, # max time of 1 minute ml = \"LinearRidgeRegression\", # use ridge regression (the default) sel='lexicase', # use epsilon lexicase selection (the default) surv='nsga2', # use nsga-2 survival (the defaut) max_depth=6, # constrain features to depth of 6 max_dim=min([X.shape[1]*2,50]), # limit representation dimensionality random_state=random_state, backprop=True, # use gradient descent to optimize weights iters=10, n_threads=4, # max 1 threads verbosity=2, # verbose output logfile='feat_'+dataset+'.log', # save a log file of the training loss print_pop=1 # print the final population ) # train the model fest.fit(X_t,y_t) # get the test score test_score = {} test_score['feat'] = mse(y_v,fest.predict(X_v)) # store the archive str_arc = fest.get_archive() For comparison, we can fit an Elastic Net and Random Forest regression model to the same data: # random forest rf = RandomForestRegressor(random_state=987039487) rf.fit(X_t,y_t) test_score['rf'] = mse(y_v,rf.predict(X_v)) # elastic net linest = ElasticNet() linest.fit(X_t,y_t) # test_score={} test_score['elasticnet'] = mse(y_v,linest.predict(X_v)) Visualizing the Archive \u00b6 Let\u2019s visualize this archive with the test scores. This gives us a sense of how increasing the representation complexity affects the quality of the model and its generalization. import numpy as np import matplotlib import matplotlib.pyplot as plt import seaborn as sns matplotlib.rcParams['figure.figsize'] = (10, 6) %matplotlib inline sns.set_style('white') complexity = [] fit_train = [] fit_val = [] fit_test = [] eqn = [] h = plt.figure(figsize=(10,6)) # store archive data from string for s in str_arc.split('\\n')[1:-1]: line = s.split('\\t') complexity.append(int(line[0])) fit_train.append(float(line[1])) fit_test.append(float(line[2])) eqn.append(','.join(line[3:])) eqn[-1].replace('sqrt','\\sqrt') # plot archive points plt.plot(fit_train,complexity,'--ro',label='Train',markersize=6) plt.plot(fit_test,complexity,'--bx',label='Validation') best = np.argmin(np.array(fit_test)) print('best:',complexity[best]) plt.plot(fit_test[best],complexity[best],'sk',markersize=16, markerfacecolor='none',label='Model Selection') # test score lines y1 = -1 y2 = 58 plt.plot((test_score['feat'],test_score['feat']),(y1,y2), '--k',label='FEAT Test',alpha=0.5) plt.plot((test_score['rf'],test_score['rf']),(y1,y2), '-.xg',label='RF Test',alpha=0.5) plt.plot((test_score['elasticnet'],test_score['elasticnet']),(y1,y2), '-sm',label='ElasticNet Test',alpha=0.5) print('complexity',complexity) eqn[best] = '0)]$\\n$'.join(eqn[best].split('0)]')) # complexity[-1] = complexity[-1]-10 # adjust placement of last equation xoff = 70 for e,t,c in zip(eqn,fit_test,complexity): if c in [1,5,12,31,43,53]: if c == 5 or c == 1: t = t+200 if c==complexity[best]: tax = plt.text(t+18000,c-5,'$\\leftarrow'+e+'$', size=18,horizontalalignment='right') tax.set_bbox(dict(facecolor='white', alpha=0.75, edgecolor='none')) elif c == 43: plt.text(t+xoff,c-1,'$\\leftarrow$ overfitting',size=18) else: tax = plt.text(t+xoff,c-1,'$\\leftarrow'+e+'$',size=18) tax.set_bbox(dict(facecolor='white', alpha=0.75, edgecolor='none')) l = plt.legend(prop={'size': 16},loc=[0.72,0.05]) plt.xlabel('MSE',size=16) # plt.gca().set_ylim(10,200) plt.gca().set_xlim(150,right=20000) # plt.gca().set_yscale('log') plt.gca().set_xscale('log') # plt.ylim(y1,y2) plt.gca().set_yticklabels('') plt.gca().set_xticklabels('') plt.ylabel('Complexity',size=18) h.tight_layout() h.savefig('archive_example.svg') plt.show() This produces the figure below. Note that ElasticNet produces a similar test score to the linear representation in Feat\u2019s archive, and that Random Forest\u2019s test score is near the representation $[(x_2 + x_0)][(x1-x_3)][\\sin(x)]$. The best model, marked with a square, is selected from the validation curve (blue line). The validation curve shows how models begin to overfit as complexity grows. By visualizing the archive, we can see that a lower complexity model (the representation beginning with $relu(x_3)$) achieves nearly as good of a validation score. In this case it may be preferable to choose that representation instead. We can also see that the validation scores in general oscillate around the test score of the final model.","title":"Using the archive"},{"location":"examples/archive/#using-the-archive","text":"In this example, we apply Feat to a regression problem and visualize the archive of representations. Note: this code uses the Penn ML Benchmark Suite to fetch data. You can install it using pip install pmlb . Also available as a notebook","title":"Using the Archive"},{"location":"examples/archive/#training-feat","text":"First, we import the data and create a train-test split. from pmlb import fetch_data from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error as mse import numpy as np dataset='690_visualizing_galaxy' X, y = fetch_data(dataset,return_X_y=True) X_t,X_v, y_t, y_v = train_test_split(X,y,train_size=0.75,test_size=0.25,random_state=42) Then we set up a Feat instance and train the model, storing the final archive. from feat import Feat # fix the random state random_state=11314 fest = Feat(pop_size=500, # train 500 representations gens=100, # maximum of 200 generations max_time=60, # max time of 1 minute ml = \"LinearRidgeRegression\", # use ridge regression (the default) sel='lexicase', # use epsilon lexicase selection (the default) surv='nsga2', # use nsga-2 survival (the defaut) max_depth=6, # constrain features to depth of 6 max_dim=min([X.shape[1]*2,50]), # limit representation dimensionality random_state=random_state, backprop=True, # use gradient descent to optimize weights iters=10, n_threads=4, # max 1 threads verbosity=2, # verbose output logfile='feat_'+dataset+'.log', # save a log file of the training loss print_pop=1 # print the final population ) # train the model fest.fit(X_t,y_t) # get the test score test_score = {} test_score['feat'] = mse(y_v,fest.predict(X_v)) # store the archive str_arc = fest.get_archive() For comparison, we can fit an Elastic Net and Random Forest regression model to the same data: # random forest rf = RandomForestRegressor(random_state=987039487) rf.fit(X_t,y_t) test_score['rf'] = mse(y_v,rf.predict(X_v)) # elastic net linest = ElasticNet() linest.fit(X_t,y_t) # test_score={} test_score['elasticnet'] = mse(y_v,linest.predict(X_v))","title":"Training Feat"},{"location":"examples/archive/#visualizing-the-archive","text":"Let\u2019s visualize this archive with the test scores. This gives us a sense of how increasing the representation complexity affects the quality of the model and its generalization. import numpy as np import matplotlib import matplotlib.pyplot as plt import seaborn as sns matplotlib.rcParams['figure.figsize'] = (10, 6) %matplotlib inline sns.set_style('white') complexity = [] fit_train = [] fit_val = [] fit_test = [] eqn = [] h = plt.figure(figsize=(10,6)) # store archive data from string for s in str_arc.split('\\n')[1:-1]: line = s.split('\\t') complexity.append(int(line[0])) fit_train.append(float(line[1])) fit_test.append(float(line[2])) eqn.append(','.join(line[3:])) eqn[-1].replace('sqrt','\\sqrt') # plot archive points plt.plot(fit_train,complexity,'--ro',label='Train',markersize=6) plt.plot(fit_test,complexity,'--bx',label='Validation') best = np.argmin(np.array(fit_test)) print('best:',complexity[best]) plt.plot(fit_test[best],complexity[best],'sk',markersize=16, markerfacecolor='none',label='Model Selection') # test score lines y1 = -1 y2 = 58 plt.plot((test_score['feat'],test_score['feat']),(y1,y2), '--k',label='FEAT Test',alpha=0.5) plt.plot((test_score['rf'],test_score['rf']),(y1,y2), '-.xg',label='RF Test',alpha=0.5) plt.plot((test_score['elasticnet'],test_score['elasticnet']),(y1,y2), '-sm',label='ElasticNet Test',alpha=0.5) print('complexity',complexity) eqn[best] = '0)]$\\n$'.join(eqn[best].split('0)]')) # complexity[-1] = complexity[-1]-10 # adjust placement of last equation xoff = 70 for e,t,c in zip(eqn,fit_test,complexity): if c in [1,5,12,31,43,53]: if c == 5 or c == 1: t = t+200 if c==complexity[best]: tax = plt.text(t+18000,c-5,'$\\leftarrow'+e+'$', size=18,horizontalalignment='right') tax.set_bbox(dict(facecolor='white', alpha=0.75, edgecolor='none')) elif c == 43: plt.text(t+xoff,c-1,'$\\leftarrow$ overfitting',size=18) else: tax = plt.text(t+xoff,c-1,'$\\leftarrow'+e+'$',size=18) tax.set_bbox(dict(facecolor='white', alpha=0.75, edgecolor='none')) l = plt.legend(prop={'size': 16},loc=[0.72,0.05]) plt.xlabel('MSE',size=16) # plt.gca().set_ylim(10,200) plt.gca().set_xlim(150,right=20000) # plt.gca().set_yscale('log') plt.gca().set_xscale('log') # plt.ylim(y1,y2) plt.gca().set_yticklabels('') plt.gca().set_xticklabels('') plt.ylabel('Complexity',size=18) h.tight_layout() h.savefig('archive_example.svg') plt.show() This produces the figure below. Note that ElasticNet produces a similar test score to the linear representation in Feat\u2019s archive, and that Random Forest\u2019s test score is near the representation $[(x_2 + x_0)][(x1-x_3)][\\sin(x)]$. The best model, marked with a square, is selected from the validation curve (blue line). The validation curve shows how models begin to overfit as complexity grows. By visualizing the archive, we can see that a lower complexity model (the representation beginning with $relu(x_3)$) achieves nearly as good of a validation score. In this case it may be preferable to choose that representation instead. We can also see that the validation scores in general oscillate around the test score of the final model.","title":"Visualizing the Archive"},{"location":"examples/ex_command_line/","text":"Command line example \u00b6 Feat can be run from the command-line. All of its options are configurable there. After a default build, the feat executable will be in the build directory. From the repo directory, type ./build/feat -h to see options. The first argument to the executable should be the dataset file to learn from. This dataset should be a comma- or tab-delimited file with columns corresponding to features, one column corresponding to the target, and rows corresponding to samples. the first row should be a header with the names of the features and target. The target must be named as class, target, or label in order to be interpreted correctly. See the datasets in the examples folder for guidance. ENC problem \u00b6 We will run Feat on the energy efficiency dataset from UCI, which is included in examples/d_enc.txt . To run Feat with a population 1000 for 100 generations using a random seed of 42, type ./build/feat examples/d_enc.csv -p 100 -g 100 -r 42 The default verbosity=1, so you will get a printout of summary statistics each generation. The final output should look like Generation 100/100 [//////////////////////////////////////////////////] Min Loss Median Loss Median (Max) Size Time (s) 2.47920e+00 6.27829e+00 27 (37) 27.75276 Representation Pareto Front-------------------------------------- Rank Complexity Loss Representation 1 1 1.65439e+01 [x_4] 1 2 1.25723e+01 [x_0][x_4] 1 3 1.03673e+01 [x_0][x_4][x_6] 1 4 1.01868e+01 [x_0][x_3][x_4][x_6] 1 5 1.00550e+01 [x_0][x_1][x_2][x_4][x_6] 1 6 9.98319e+00 [x_0][x_1][x_2][x_4][x_6][x_7] 1 7 9.78112e+00 [(x_1^2)][x_2][x_4][x_6] 1 8 9.70928e+00 [x_7][(x_1^2)][x_2][x_4][x_6] 1 9 9.68727e+00 [x_7][(x_1^2)][x_2][x_3][x_4][x_6] 1 10 9.33169e+00 [x_1][(x_2*x_0)][x_2][x_4][x_6] 1 11 9.32995e+00 [x_1][(x_2*x_0)][x_2][x_4][x_5][x_6] 1 12 8.84481e+00 [x_2][(x_1^2)][x_2][x_3][x_4][x_6][(x_3+x_0)] 1 14 8.08357e+00 [x_6][(x_2/(x_2^2))][x_4] 1 15 8.08357e+00 [x_6][(x_2/(x_2^2))][x_4][x_4] 1 17 7.95839e+00 [x_6][(x_2/(x_2^2))][x_4][(x_0-x_4)] 1 20 7.87072e+00 [x_6][(x_2/(x_2^2))][x_4][(x_6*x_7)] 1 21 7.55717e+00 [(x_2/(x_2^2))][(x_1+(x_6+x_2))][(x_0-x_4)][x_1] 1 22 7.47840e+00 [x_6][(x_2/(x_2^2))][x_4][log(x_1)] 1 23 4.84868e+00 [(x_2/(x_2^2))][(x_1^2)][x_2][x_3][x_4][x_6][(x_3+x_0)] 1 42 4.25015e+00 [((x_1+x_4)^2)][(x_2/x_1)][(x_6+(x_6+x_2))][(x_1-x_2)][((x_1^2)^2)][(x_0^2)][(x_0*x_2)] 1 45 4.23304e+00 [((x_1+x_4)^2)][(x_2/x_1)][(x_6+(x_6+x_2))][(x_1-x_2)][((x_1^2)^2)][(x_0^2)][(x_0*x_2)][(x_1-x_5)] 1 48 3.78240e+00 [((x_1+x_4)^2)][(x_2/x_1)][(x_6+(x_6+x_2))][(x_1-x_2)][((x_1^2)^2)][(x_0^2)][(x_0*x_2)][(x_4+x_3)][(x_1==x_4)] 1 64 3.02734e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)] 1 67 2.96073e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)][(x_4-x_7)] 1 70 2.76775e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)][(x_6*x_7)] 1 73 2.75930e+00 [x_1][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)][(x_6*x_7)][(x_1-x_2)] 1 74 2.75432e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)][(x_6*x_7)][(x_6^2)] 1 78 2.74151e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)][(x_6*x_7)][exp(x_4)] 1 80 2.57538e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_6^2)^2)][((x_1^2)^2)][((x_1+x_4)^2)][(x_6*x_4)][(x_0^2)] 1 115 2.52790e+00 [((exp(x_3)^2)^2)][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_0^2))][(x_6+x_2)][((x_6^2)^2)][((x_1^2)^2)][((x_1+x_4)^2)][(x_6*x_4)][(x_0^2)] 1 117 2.49092e+00 [((exp(x_3)^2)^2)][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_0*(x_0^2))][(x_6+(x_1+x_2))][((x_6^2)^2)][((x_1^2)^2)][((x_1+x_4)^2)][(x_6*x_4)][(x_0^2)] 1 129 2.47920e+00 [((exp(x_3)^2)^2)][(x_2/x_1)][((x_4^2)/((x_2^2)^2))][(x_1*(x_0^2))][(x_6+(x_1+x_2))][((x_6^2)^2)][((x_1^2)^2)][((x_1+x_4)^2)][(x_6*x_4)][(x_0^2)] finished best training representation: [((exp(x_3)^2)^2)][(x_2/x_1)][((x_4^2)/((x_2^2)^2))][(x_1*(x_0^2))][(x_6+(x_1+x_2))][((x_6^2)^2)][((x_1^2)^2)][((x_1+x_4)^2)][(x_6*x_4)][(x_0^2)] train score: 2.479198 best validation representation: [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)] validation score: 3.596077 final_model score: 3.202949 generating training prediction... predicting with best_ind train score: 3.20295e+00 generating test prediction... predicting with best_ind test score: 4.24600e+00 done! tab-delimited csv files \u00b6 When using tab-delimited csv files as input, specify -sep \\\\t or -sep \"\\t\" at the command line. Feat Cross Validator \u00b6 The cross-validation version of Feat named feat_cv is also built and present in the build/ directory. For cross validation, there are several hyperparameters of Feat that can be tuned: pop_size generations ml max_stall selection survival cross_rate functions max_depth max_dim erc objectives feedback There are 2 ways to set the hyper parameters for feat_cv. The first method is to define a string in cv_main.cc file and pass that to the feat_cv constructor. See cv_main.cc for details. The second method is to create a input file containing group of parameters and pass the filepath using -infile flag. Check featcvinput.txt for a sample input file. The input file contains a string similar to the one in cv_main.cc . The general structure of input file is [{'token1': (val1, val2, val3), 'token2': (val1, val2)}, {'token1': (val1, val2, val3), 'token2': (val1, val2)}]","title":"Command line"},{"location":"examples/ex_command_line/#command-line-example","text":"Feat can be run from the command-line. All of its options are configurable there. After a default build, the feat executable will be in the build directory. From the repo directory, type ./build/feat -h to see options. The first argument to the executable should be the dataset file to learn from. This dataset should be a comma- or tab-delimited file with columns corresponding to features, one column corresponding to the target, and rows corresponding to samples. the first row should be a header with the names of the features and target. The target must be named as class, target, or label in order to be interpreted correctly. See the datasets in the examples folder for guidance.","title":"Command line example"},{"location":"examples/ex_command_line/#enc-problem","text":"We will run Feat on the energy efficiency dataset from UCI, which is included in examples/d_enc.txt . To run Feat with a population 1000 for 100 generations using a random seed of 42, type ./build/feat examples/d_enc.csv -p 100 -g 100 -r 42 The default verbosity=1, so you will get a printout of summary statistics each generation. The final output should look like Generation 100/100 [//////////////////////////////////////////////////] Min Loss Median Loss Median (Max) Size Time (s) 2.47920e+00 6.27829e+00 27 (37) 27.75276 Representation Pareto Front-------------------------------------- Rank Complexity Loss Representation 1 1 1.65439e+01 [x_4] 1 2 1.25723e+01 [x_0][x_4] 1 3 1.03673e+01 [x_0][x_4][x_6] 1 4 1.01868e+01 [x_0][x_3][x_4][x_6] 1 5 1.00550e+01 [x_0][x_1][x_2][x_4][x_6] 1 6 9.98319e+00 [x_0][x_1][x_2][x_4][x_6][x_7] 1 7 9.78112e+00 [(x_1^2)][x_2][x_4][x_6] 1 8 9.70928e+00 [x_7][(x_1^2)][x_2][x_4][x_6] 1 9 9.68727e+00 [x_7][(x_1^2)][x_2][x_3][x_4][x_6] 1 10 9.33169e+00 [x_1][(x_2*x_0)][x_2][x_4][x_6] 1 11 9.32995e+00 [x_1][(x_2*x_0)][x_2][x_4][x_5][x_6] 1 12 8.84481e+00 [x_2][(x_1^2)][x_2][x_3][x_4][x_6][(x_3+x_0)] 1 14 8.08357e+00 [x_6][(x_2/(x_2^2))][x_4] 1 15 8.08357e+00 [x_6][(x_2/(x_2^2))][x_4][x_4] 1 17 7.95839e+00 [x_6][(x_2/(x_2^2))][x_4][(x_0-x_4)] 1 20 7.87072e+00 [x_6][(x_2/(x_2^2))][x_4][(x_6*x_7)] 1 21 7.55717e+00 [(x_2/(x_2^2))][(x_1+(x_6+x_2))][(x_0-x_4)][x_1] 1 22 7.47840e+00 [x_6][(x_2/(x_2^2))][x_4][log(x_1)] 1 23 4.84868e+00 [(x_2/(x_2^2))][(x_1^2)][x_2][x_3][x_4][x_6][(x_3+x_0)] 1 42 4.25015e+00 [((x_1+x_4)^2)][(x_2/x_1)][(x_6+(x_6+x_2))][(x_1-x_2)][((x_1^2)^2)][(x_0^2)][(x_0*x_2)] 1 45 4.23304e+00 [((x_1+x_4)^2)][(x_2/x_1)][(x_6+(x_6+x_2))][(x_1-x_2)][((x_1^2)^2)][(x_0^2)][(x_0*x_2)][(x_1-x_5)] 1 48 3.78240e+00 [((x_1+x_4)^2)][(x_2/x_1)][(x_6+(x_6+x_2))][(x_1-x_2)][((x_1^2)^2)][(x_0^2)][(x_0*x_2)][(x_4+x_3)][(x_1==x_4)] 1 64 3.02734e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)] 1 67 2.96073e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)][(x_4-x_7)] 1 70 2.76775e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)][(x_6*x_7)] 1 73 2.75930e+00 [x_1][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)][(x_6*x_7)][(x_1-x_2)] 1 74 2.75432e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)][(x_6*x_7)][(x_6^2)] 1 78 2.74151e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)][(x_6*x_7)][exp(x_4)] 1 80 2.57538e+00 [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_6^2)^2)][((x_1^2)^2)][((x_1+x_4)^2)][(x_6*x_4)][(x_0^2)] 1 115 2.52790e+00 [((exp(x_3)^2)^2)][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_0^2))][(x_6+x_2)][((x_6^2)^2)][((x_1^2)^2)][((x_1+x_4)^2)][(x_6*x_4)][(x_0^2)] 1 117 2.49092e+00 [((exp(x_3)^2)^2)][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_0*(x_0^2))][(x_6+(x_1+x_2))][((x_6^2)^2)][((x_1^2)^2)][((x_1+x_4)^2)][(x_6*x_4)][(x_0^2)] 1 129 2.47920e+00 [((exp(x_3)^2)^2)][(x_2/x_1)][((x_4^2)/((x_2^2)^2))][(x_1*(x_0^2))][(x_6+(x_1+x_2))][((x_6^2)^2)][((x_1^2)^2)][((x_1+x_4)^2)][(x_6*x_4)][(x_0^2)] finished best training representation: [((exp(x_3)^2)^2)][(x_2/x_1)][((x_4^2)/((x_2^2)^2))][(x_1*(x_0^2))][(x_6+(x_1+x_2))][((x_6^2)^2)][((x_1^2)^2)][((x_1+x_4)^2)][(x_6*x_4)][(x_0^2)] train score: 2.479198 best validation representation: [x_3][(x_2/x_1)][((x_4^2)/(x_2^2))][(x_1*(x_1^2))][(x_6+(x_1+x_2))][((x_1^2)^2)][((x_1+x_4)^2)][(x_0^2)] validation score: 3.596077 final_model score: 3.202949 generating training prediction... predicting with best_ind train score: 3.20295e+00 generating test prediction... predicting with best_ind test score: 4.24600e+00 done!","title":"ENC problem"},{"location":"examples/ex_command_line/#tab-delimited-csv-files","text":"When using tab-delimited csv files as input, specify -sep \\\\t or -sep \"\\t\" at the command line.","title":"tab-delimited csv files"},{"location":"examples/ex_command_line/#feat-cross-validator","text":"The cross-validation version of Feat named feat_cv is also built and present in the build/ directory. For cross validation, there are several hyperparameters of Feat that can be tuned: pop_size generations ml max_stall selection survival cross_rate functions max_depth max_dim erc objectives feedback There are 2 ways to set the hyper parameters for feat_cv. The first method is to define a string in cv_main.cc file and pass that to the feat_cv constructor. See cv_main.cc for details. The second method is to create a input file containing group of parameters and pass the filepath using -infile flag. Check featcvinput.txt for a sample input file. The input file contains a string similar to the one in cv_main.cc . The general structure of input file is [{'token1': (val1, val2, val3), 'token2': (val1, val2)}, {'token1': (val1, val2, val3), 'token2': (val1, val2)}]","title":"Feat Cross Validator"},{"location":"guide/basics/","text":"","title":"Basics"},{"location":"guide/configuration/","text":"","title":"Configuring Feat"},{"location":"guide/data/","text":"","title":"Data"}]}